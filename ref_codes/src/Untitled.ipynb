{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 分析如何Hook \n",
    "建议建在example里面"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "example (generic function with 1 method)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function example(example_f_case, T)\n",
    "    A, B, y, u, C, D = example_f_case()\n",
    "    Y, U = trajectory_H(T, A, B, C, D, y, u) # Generate simulation data of T \n",
    "    # Add\n",
    "    Y_out, Y_in, U_in = format_ols(Y, U) # Prepare\n",
    "    ols(Y_out, Y_in, U_in) # Conduct ols\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using StatsBase\n",
    "import StatsBase: predict # Import to override\n",
    "\n",
    "using Printf, LinearAlgebra, Random, Distributions\n",
    "using JuMP, Ipopt\n",
    "\n",
    "export svm, predict, cost, accuracy, pegasos\n",
    "\n",
    "struct SVMFit\n",
    "    w::Vector{Float64}\n",
    "    λ::Float32\n",
    "    passes::Union{Int32, Nothing}\n",
    "    converged::Union{Bool, Nothing}\n",
    "    optimal::Union{Bool, Nothing}\n",
    "end\n",
    "\n",
    "function predict(fit::SVMFit, X::AbstractMatrix{<:Real})\n",
    "    n, m = size(X)\n",
    "    preds = fill(0, m)\n",
    "    for i in 1:m\n",
    "        dot_prod = 0.0\n",
    "        for j in 1:n\n",
    "            dot_prod += fit.w[j] * X[j, i]\n",
    "        end\n",
    "        preds[i] = sign(dot_prod) # x 的正负号，返回值为-1, 0, 或+1\n",
    "    end\n",
    "    return preds\n",
    "end\n",
    "\n",
    "@enum Optimizer begin\n",
    "    PegasosBatch\n",
    "    CDDual\n",
    "    InteriorPoint\n",
    "end\n",
    "\n",
    "function svm(X::AbstractMatrix{<:Real},\n",
    "             Y::AbstractVector{<:Real};\n",
    "             optimizer::Optimizer,\n",
    "             λ::Real = 0.1,\n",
    "             ϵ::Real = 1e-6,\n",
    "             max_passes::Integer = 100)\n",
    "    svmFit = undef\n",
    "    if optimizer == PegasosBatch\n",
    "        (w, t_converge) = pegasos_batch(X, Y, lambda = λ, maxpasses = max_passes)\n",
    "        svmFit = SVMFit(w, Float32(λ), t_converge, t_converge < Inf, nothing)\n",
    "    elseif optimizer == CDDual\n",
    "        (w, t_converge) = cddual(X, Y, norm = 1, maxpasses = max_passes)\n",
    "        svmFit = SVMFit(w, Float32(λ), t_converge, t_converge < Inf, nothing)\n",
    "    elseif optimizer == InteriorPoint\n",
    "        (w, optimal) = interiorPoint(X, Y; λ = λ)\n",
    "        svmFit = SVMFit(w, Float32(λ), nothing, nothing, optimal)\n",
    "    else\n",
    "        error(\"unsupported optimizer type: $(optimizer)\")\n",
    "    end\n",
    "\n",
    "    return svmFit\n",
    "end\n",
    "\n",
    "function cost(fit::SVMFit,\n",
    "              X::AbstractMatrix{<:Real},\n",
    "              Y::AbstractVector{<:Real})\n",
    "    w = fit.w\n",
    "    λ = fit.λ\n",
    "    n, m = size(X)\n",
    "    risk = λ / 2 * norm(w, 2)^2\n",
    "    for i in 1:m\n",
    "        p = 0.0\n",
    "        l(w, x, y) = max(0, 1 - y * (w ⋅ x))\n",
    "        risk += l(w, X[:, i], Y[i])\n",
    "    end\n",
    "    return risk / m\n",
    "end\n",
    "\n",
    "\n",
    "function accuracy(fit::SVMFit,\n",
    "                  X::AbstractMatrix{<:Real},\n",
    "                  Y::AbstractVector{<:Real})\n",
    "    n, m = size(X)\n",
    "    return count(predict(fit, X) .== Y) / m\n",
    "end\n",
    "\n",
    "\n",
    "function interiorPoint(X::AbstractMatrix{<:Real},\n",
    "                       Y::AbstractVector{<:Real};\n",
    "                       λ::Real = 0.1)\n",
    "    n, m = size(X)\n",
    "\n",
    "    model = Model(with_optimizer(Ipopt.Optimizer))\n",
    "    @variable(model, w[1:n])\n",
    "    @objective(model, Min, w ⋅ w)\n",
    "    for i in 1:m\n",
    "        @constraint(model, Y[i] * (w ⋅ X[:, i]) ≥ 1)\n",
    "    end\n",
    "\n",
    "    optimize!(model)\n",
    "\n",
    "    w_sol, optimal = undef, undef # hack, indication of solution status\n",
    "    if termination_status(model) == MOI.OPTIMAL\n",
    "        w_sol = value.(w)\n",
    "        optimal = true\n",
    "    elseif has_values(model)\n",
    "        w_sol = value.(w)\n",
    "        optimal = false\n",
    "    #else\n",
    "        #warn(\"The model was not solved correctly.\")\n",
    "    end\n",
    "\n",
    "    return (w_sol, optimal)\n",
    "end\n",
    "\n",
    "\n",
    "\n",
    "function pegasos(X::AbstractMatrix{<:Real},\n",
    "                 Y::AbstractVector{<:Real};\n",
    "                 λ::Real = 0.1,\n",
    "                 ϵ::Real = 1e-3,\n",
    "                 T::Integer = 100,\n",
    "                 seed::Integer = 123)\n",
    "    rng = MersenneTwister(seed)\n",
    "\n",
    "    n, m = size(X)\n",
    "\n",
    "    # Initialize weights to 0\n",
    "    w = zeros(n)\n",
    "    I = DiscreteUniform(1, m)\n",
    "\n",
    "    t_converge = Inf\n",
    "\n",
    "    # Iterations\n",
    "    for t in 1:T\n",
    "        i = rand(rng, I)\n",
    "        η = 1 / (λ * t)\n",
    "        (x_i, y_i) = (X[:, i], Y[i])\n",
    "        p = y_i * (w ⋅ x_i)\n",
    "        ∇_w = nothing\n",
    "        if p ≥ 1\n",
    "           ∇_w = λ * w\n",
    "        else # p < 1\n",
    "           ∇_w = λ * w - y_i * x_i\n",
    "        end\n",
    "        w = w - η * ∇_w\n",
    "        if t % 10 == 0\n",
    "            @show norm(η * ∇_w, 2)\n",
    "            @show cost(SVMFit(w, λ, T, t_converge < Inf), X, Y)\n",
    "        end\n",
    "\n",
    "#        if norm(η * ∇_w, 2) < ϵ\n",
    "#            t_converge = t\n",
    "#            break\n",
    "#        end\n",
    "    end\n",
    "\n",
    "    return (w, t_converge)\n",
    "end\n",
    "\n",
    "# S is X,Y\n",
    "# T is maxpasses\n",
    "# p: # of features\n",
    "# n: # of data points\n",
    "# k: size of minibatch\n",
    "function pegasos_batch(X::AbstractMatrix{<:Real},\n",
    "                          Y::AbstractVector{<:Real};\n",
    "                          k::Integer = 5,\n",
    "                          lambda::Real = 0.1,\n",
    "                          maxpasses::Integer = 100)\n",
    "    # p features, n observations\n",
    "    p, n = size(X)\n",
    "\n",
    "    # Initialize weights so norm(w) <= 1 / sqrt(lambda)\n",
    "    w = randn(p)\n",
    "    sqrtlambda = sqrt(lambda)\n",
    "    normalizer = sqrtlambda * norm(w)\n",
    "    for j in 1:p\n",
    "        w[j] /= normalizer\n",
    "    end\n",
    "\n",
    "    # Allocate storage for repeated used arrays\n",
    "    deltaw = Array{Float64,1}(undef, p)\n",
    "    w_tmp = Array{Float64,1}(undef, p)\n",
    "\n",
    "    # Loop\n",
    "    for t in 1:maxpasses\n",
    "        # Calculate stepsize parameters\n",
    "        alpha = 1.0 / t\n",
    "        eta_t = 1.0 / (lambda * t)\n",
    "\n",
    "        # Calculate scaled sum over misclassified examples\n",
    "        # Subgradient over minibatch of size k\n",
    "        fill!(deltaw, 0.0)\n",
    "        for i in 1:k\n",
    "            # Select a random item from X\n",
    "            # This is one element of At of S\n",
    "            index = rand(1:n)\n",
    "\n",
    "            # Test if prediction isn't sufficiently good\n",
    "            # If so, current item is element of At+\n",
    "            pred = Y[index] * dot(w, X[:, index])\n",
    "            if pred < 1.0\n",
    "                # Update subgradient\n",
    "                for j in 1:p\n",
    "                    deltaw[j] += Y[index] * X[j, index]\n",
    "                end\n",
    "            end\n",
    "        end\n",
    "\n",
    "        # Rescale subgradient\n",
    "        for j in 1:p\n",
    "            deltaw[j] *= (eta_t / k)\n",
    "        end\n",
    "\n",
    "        # Calculate tentative weight-update\n",
    "        for j in 1:p\n",
    "            w_tmp[j] = (1.0 - alpha) * w[j] + deltaw[j]\n",
    "        end\n",
    "\n",
    "        # Find projection of weights into L2 ball\n",
    "        proj = min(1.0, 1.0 / (sqrtlambda * norm(w_tmp)))\n",
    "        for j in 1:p\n",
    "            w[j] = proj * w_tmp[j]\n",
    "        end\n",
    "    end\n",
    "\n",
    "    return (w, maxpasses)\n",
    "end\n",
    "\n",
    "# Randomization option slows down processing\n",
    "# but improves quality of solution considerably\n",
    "# Would be better to do randomization in place\n",
    "function cddual(X::AbstractMatrix{<:Real},\n",
    "                Y::AbstractVector{<:Real};\n",
    "                C::Real = 1.0,\n",
    "                norm::Integer = 2,\n",
    "                randomized::Bool = true,\n",
    "                maxpasses::Integer = 2)\n",
    "    # l: # of samples\n",
    "    # n: # of features\n",
    "    n, l = size(X)\n",
    "    alpha = zeros(Float64, l)\n",
    "    w = zeros(Float64, n)\n",
    "\n",
    "    # Set U and D\n",
    "    #  * L1-SVM: U = C, D[i] = 0\n",
    "    #  * L2-SVM: U = Inf, D[i] = 1 / (2C)\n",
    "    U = 0.0\n",
    "    if norm == 1\n",
    "        U = C\n",
    "        D = zeros(Float64, l)\n",
    "    elseif norm == 2\n",
    "        U = Inf\n",
    "        D = fill(1.0 / (2.0 * C), l)\n",
    "    else\n",
    "        throw(ArgumentError(\"Only L1-SVM and L2-SVM are supported\"))\n",
    "    end\n",
    "\n",
    "    # Set Qbar\n",
    "    Qbar = Array{Float64,1}(undef, l)\n",
    "    for i in 1:l\n",
    "        Qbar[i] = D[i] + dot(X[:, i], X[:, i])\n",
    "    end\n",
    "\n",
    "    # Loop over examples\n",
    "    converged = false\n",
    "    pass = 0\n",
    "\n",
    "    while !converged\n",
    "        # Assess convergence\n",
    "        pass += 1\n",
    "        if pass == maxpasses\n",
    "            converged = true\n",
    "        end\n",
    "\n",
    "        # Choose order of observations to process\n",
    "        if randomized\n",
    "            indices = randperm(l)\n",
    "        else\n",
    "            indices = 1:l\n",
    "        end\n",
    "\n",
    "        # Process all observations\n",
    "        for i in indices\n",
    "            g = Y[i] * dot(w, X[:, i]) - 1.0 + D[i] * alpha[i]\n",
    "\n",
    "            if alpha[i] == 0.0\n",
    "                pg = min(g, 0.0)\n",
    "            elseif alpha[i] == U\n",
    "                pg = max(g, 0.0)\n",
    "            else\n",
    "                pg = g\n",
    "            end\n",
    "\n",
    "            if abs(pg) > 0.0\n",
    "                alphabar = alpha[i]\n",
    "                alpha[i] = min(max(alpha[i] - g / Qbar[i], 0.0), U)\n",
    "                for j in 1:n\n",
    "                    w[j] = w[j] + (alpha[i] - alphabar) * Y[i] * X[j, i]\n",
    "                end\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "\n",
    "    return (w, pass)\n",
    "end\n",
    "\n",
    "\n",
    "end # module SVM"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.6.1",
   "language": "julia",
   "name": "julia-1.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
